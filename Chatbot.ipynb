{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c326ce-0d14-4dfe-8eec-ff68d76ffcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HBK\\anaconda3\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re     # stand for Regular expression operations\n",
    "import nltk   # Natural Language Toolkit\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3863cacf-853e-445f-9885-00fd8cf482ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9700c288-d381-4689-baef-feaa6dcbceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f14d30d0-0cb2-4da6-82a2-143fc143c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = []\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "194fe07d-5990-4cf9-a851-6caa76c80bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L929 What just happened?\n",
      "L930 Your daughters went to the prom.\n",
      "L931 Did I have anything to say about it?\n",
      "L932 Absolutely not.\n",
      "L933 That ' s what I thought\n"
     ]
    }
   ],
   "source": [
    "#id and conversation sample\n",
    "for k in convs[200]:\n",
    "    print (k, id2line[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "124f491e-ac31-41c4-8eba-02da37605605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])\n",
    "        \n",
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31995136-274a-427b-8fe2-94e126214146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2082687f-8849-4607-afc0-f62f91cae4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71f08174-0d5d-474b-bcdf-d19a4e854cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "19.0\n",
      "24.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "# Find the length of sentences (not using nltk due to processing speed)\n",
    "lengths = []\n",
    "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
    "print(np.percentile(lengths, 80))\n",
    "print(np.percentile(lengths, 85))\n",
    "print(np.percentile(lengths, 90))\n",
    "print(np.percentile(lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d77f23cc-9746-437d-bf25-8b12db7aa9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138528\n",
      "138528\n"
     ]
    }
   ],
   "source": [
    "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "for i, question in enumerate(clean_questions):\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "for i, answer in enumerate(short_answers_temp):\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "        \n",
    "print(len(short_questions))\n",
    "print(len(short_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4a4c9f6-3319-4180-8396-a8f338be5cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well, i, uh, i will be leaving town for a little while.\n",
      "for how long?\n",
      "\n",
      "for how long?\n",
      "oh, i do not know.\n",
      "\n",
      "oh, i do not know.\n",
      "a week? a year?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = np.random.randint(1,len(short_questions))\n",
    "\n",
    "for i in range(r, r+3):\n",
    "    print(short_questions[i])\n",
    "    print(short_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2d52c9d-af0a-4b4f-98fa-9c90df19048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 90000  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f290d8a-4c8a-4b0e-bd70-a142738289e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 72000\n",
      "validation size 18000\n"
     ]
    }
   ],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ae882-026a-4cd2-b17c-f1c809b14154",
   "metadata": {},
   "source": [
    "# Word encoding decoding dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ce74d34-2d8f-4cec-858f-9ff25f92ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b2462-53c8-44cd-9bc7-58948d0945ad",
   "metadata": {},
   "source": [
    "## Remove the rare word from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9265ded1-5c02-45c7-97c1-0f324cdf4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 15\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eec56c6-dcf4-44f6-bbae-ff1a923a6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 31650\n",
      "Size of vocab we will use: 4387\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73a8ddcf-79d0-4202-a070-5fff5f9eff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocab used: 4389\n"
     ]
    }
   ],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0495dda-956b-411b-a5a1-1f975e044adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b9682b9-6a70-41df-bc16-415e4b14d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4390"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = word_num+1\n",
    "dict_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d7d29-2223-47f3-9d46-2793c465ed77",
   "metadata": {},
   "source": [
    "# Vectorizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de132edc-8636-4f29-bbd3-cff600fe047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(encoding, data, vector_size=20):\n",
    "    \"\"\"\n",
    "    :param encoding: encoding dict built by build_word_encoding()\n",
    "    :param data: list of strings\n",
    "    :param vector_size: size of each encoded vector\n",
    "    \"\"\"\n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23930973-d94c-4f3d-9de0-4d64b5b35e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (72000, 20)\n",
      "encoded_training_output (72000, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding training set\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b34171-8761-499e-a441-130199283503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_validation_input (18000, 20)\n",
      "encoded_validation_output (18000, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding validation set\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843de8ac-db7a-423c-857a-053caf2155d2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b700885a-2856-40ed-8d3c-80e16133ab44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HBK\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6b9873e-8f83-493e-ba2a-3c80c890c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47c1f5ec-1a29-41a2-956b-34cf80d42ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HBK\\anaconda3\\lib\\site-packages\\keras\\initializers\\initializers_v1.py:277: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "encoder Tensor(\"lstm/transpose_2:0\", shape=(?, 20, 256), dtype=float32)\n",
      "encoder_last Tensor(\"strided_slice:0\", shape=(?, 256), dtype=float32)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "decoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "path_checkpoint = \"model_checkpoint1.h5\"\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "modelckpt_callback = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_loss\",filepath=path_checkpoint, save_weights_only=True, save_best_only=True, )\n",
    "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(256, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)\n",
    "\n",
    "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(256, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "\n",
    "print('decoder', decoder)\n",
    "\n",
    "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
    "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be949c-8bd6-448e-8edd-c2396adfe36e",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0f72626-7018-44f8-92aa-608aafe2dda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention Tensor(\"attention/Softmax:0\", shape=(?, 20, 20), dtype=float32)\n",
      "context Tensor(\"dot_1/MatMul:0\", shape=(?, 20, 256), dtype=float32)\n",
      "decoder_combined_context Tensor(\"concatenate/concat:0\", shape=(?, 20, 512), dtype=float32)\n",
      "output Tensor(\"time_distributed_1/Reshape_1:0\", shape=(?, 20, 4390), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('attention', attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "print('context', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(256, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7431307-762b-4dd7-9693-e69be890bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 20, 128)      561920      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 20, 256)      394240      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 20, 128)      561920      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf_op_layer_strided_slice (Ten  [(None, 256)]       0           ['lstm[0][0]']                   \n",
      " sorFlowOpLayer)                                                                                  \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 20, 256)      394240      ['embedding_1[0][0]',            \n",
      "                                                                  'tf_op_layer_strided_slice[0][0]\n",
      "                                                                 ',                               \n",
      "                                                                  'tf_op_layer_strided_slice[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 20, 20)       0           ['lstm_1[0][0]',                 \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " attention (Activation)         (None, 20, 20)       0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 20, 256)      0           ['attention[0][0]',              \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 20, 512)      0           ['dot_1[0][0]',                  \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 20, 256)     131328      ['concatenate[0][0]']            \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 20, 4390)    1128230     ['time_distributed[0][0]']       \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,171,878\n",
      "Trainable params: 3,171,878\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b62b178-c1e8-460f-94d8-51892266bc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START\n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94343321-4aa3-4dfa-b896-a2ec7427e119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ee7f0b3-f330-4978-a339-92c11778b5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 18000 samples\n",
      "Epoch 1/100\n",
      "72000/72000 [==============================] - ETA: 0s - loss: 8.1956e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HBK\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000/72000 [==============================] - 232s 3ms/sample - loss: 8.1956e-04 - val_loss: 7.4223e-04\n",
      "Epoch 2/100\n",
      "72000/72000 [==============================] - 216s 3ms/sample - loss: 7.0827e-04 - val_loss: 6.8558e-04\n",
      "Epoch 3/100\n",
      "72000/72000 [==============================] - 198s 3ms/sample - loss: 6.7126e-04 - val_loss: 6.6345e-04\n",
      "Epoch 4/100\n",
      "72000/72000 [==============================] - 275s 4ms/sample - loss: 6.4885e-04 - val_loss: 6.4109e-04\n",
      "Epoch 5/100\n",
      "72000/72000 [==============================] - 296s 4ms/sample - loss: 6.3103e-04 - val_loss: 6.2692e-04\n",
      "Epoch 6/100\n",
      "72000/72000 [==============================] - 274s 4ms/sample - loss: 6.1702e-04 - val_loss: 6.1581e-04\n",
      "Epoch 7/100\n",
      "72000/72000 [==============================] - 213s 3ms/sample - loss: 6.0705e-04 - val_loss: 6.0785e-04\n",
      "Epoch 8/100\n",
      "72000/72000 [==============================] - 193s 3ms/sample - loss: 6.0027e-04 - val_loss: 6.0283e-04\n",
      "Epoch 9/100\n",
      "72000/72000 [==============================] - 255s 4ms/sample - loss: 5.9461e-04 - val_loss: 5.9878e-04\n",
      "Epoch 10/100\n",
      "72000/72000 [==============================] - 273s 4ms/sample - loss: 5.8979e-04 - val_loss: 5.9516e-04\n",
      "Epoch 11/100\n",
      "72000/72000 [==============================] - 261s 4ms/sample - loss: 5.8536e-04 - val_loss: 5.9163e-04\n",
      "Epoch 12/100\n",
      "72000/72000 [==============================] - 263s 4ms/sample - loss: 5.8150e-04 - val_loss: 5.8905e-04\n",
      "Epoch 13/100\n",
      "72000/72000 [==============================] - 205s 3ms/sample - loss: 5.7797e-04 - val_loss: 5.8613e-04\n",
      "Epoch 14/100\n",
      "72000/72000 [==============================] - 236s 3ms/sample - loss: 5.7476e-04 - val_loss: 5.8334e-04\n",
      "Epoch 15/100\n",
      "72000/72000 [==============================] - 264s 4ms/sample - loss: 5.7174e-04 - val_loss: 5.8149e-04\n",
      "Epoch 16/100\n",
      "72000/72000 [==============================] - 277s 4ms/sample - loss: 5.6907e-04 - val_loss: 5.7966e-04\n",
      "Epoch 17/100\n",
      "72000/72000 [==============================] - 242s 3ms/sample - loss: 5.6651e-04 - val_loss: 5.7769e-04\n",
      "Epoch 18/100\n",
      "72000/72000 [==============================] - 227s 3ms/sample - loss: 5.6412e-04 - val_loss: 5.7691e-04\n",
      "Epoch 19/100\n",
      "72000/72000 [==============================] - 187s 3ms/sample - loss: 5.6198e-04 - val_loss: 5.7508e-04\n",
      "Epoch 20/100\n",
      "72000/72000 [==============================] - 186s 3ms/sample - loss: 5.5984e-04 - val_loss: 5.7371e-04\n",
      "Epoch 21/100\n",
      "72000/72000 [==============================] - 331s 5ms/sample - loss: 5.5797e-04 - val_loss: 5.7292e-04\n",
      "Epoch 22/100\n",
      "72000/72000 [==============================] - 180s 2ms/sample - loss: 5.5606e-04 - val_loss: 5.7198e-04\n",
      "Epoch 23/100\n",
      "72000/72000 [==============================] - 257s 4ms/sample - loss: 5.5416e-04 - val_loss: 5.7073e-04\n",
      "Epoch 24/100\n",
      "72000/72000 [==============================] - 225s 3ms/sample - loss: 5.5235e-04 - val_loss: 5.6988e-04\n",
      "Epoch 25/100\n",
      "72000/72000 [==============================] - 187s 3ms/sample - loss: 5.5060e-04 - val_loss: 5.6882e-04\n",
      "Epoch 26/100\n",
      "72000/72000 [==============================] - 320s 4ms/sample - loss: 5.4897e-04 - val_loss: 5.6830e-04\n",
      "Epoch 27/100\n",
      "72000/72000 [==============================] - 217s 3ms/sample - loss: 5.4737e-04 - val_loss: 5.6725e-04\n",
      "Epoch 28/100\n",
      "72000/72000 [==============================] - 251s 3ms/sample - loss: 5.4582e-04 - val_loss: 5.6676e-04\n",
      "Epoch 29/100\n",
      "72000/72000 [==============================] - 258s 4ms/sample - loss: 5.4429e-04 - val_loss: 5.6617e-04\n",
      "Epoch 30/100\n",
      "72000/72000 [==============================] - 234s 3ms/sample - loss: 5.4284e-04 - val_loss: 5.6546e-04\n",
      "Epoch 31/100\n",
      "72000/72000 [==============================] - 217s 3ms/sample - loss: 5.4141e-04 - val_loss: 5.6491e-04\n",
      "Epoch 32/100\n",
      "72000/72000 [==============================] - 208s 3ms/sample - loss: 5.3999e-04 - val_loss: 5.6434e-04\n",
      "Epoch 33/100\n",
      "72000/72000 [==============================] - 228s 3ms/sample - loss: 5.3853e-04 - val_loss: 5.6441e-04\n",
      "Epoch 34/100\n",
      "72000/72000 [==============================] - 238s 3ms/sample - loss: 5.3714e-04 - val_loss: 5.6406e-04\n",
      "Epoch 35/100\n",
      "72000/72000 [==============================] - 230s 3ms/sample - loss: 5.3576e-04 - val_loss: 5.6445e-04\n",
      "Epoch 36/100\n",
      "72000/72000 [==============================] - 222s 3ms/sample - loss: 5.3443e-04 - val_loss: 5.6319e-04\n",
      "Epoch 37/100\n",
      "72000/72000 [==============================] - 252s 3ms/sample - loss: 5.3316e-04 - val_loss: 5.6292e-04\n",
      "Epoch 38/100\n",
      "72000/72000 [==============================] - 248s 3ms/sample - loss: 5.3184e-04 - val_loss: 5.6269e-04\n",
      "Epoch 39/100\n",
      "72000/72000 [==============================] - 251s 3ms/sample - loss: 5.3058e-04 - val_loss: 5.6284e-04\n",
      "Epoch 40/100\n",
      "72000/72000 [==============================] - 219s 3ms/sample - loss: 5.2933e-04 - val_loss: 5.6281e-04\n",
      "Epoch 41/100\n",
      "72000/72000 [==============================] - 214s 3ms/sample - loss: 5.2802e-04 - val_loss: 5.6176e-04\n",
      "Epoch 42/100\n",
      "72000/72000 [==============================] - 225s 3ms/sample - loss: 5.2676e-04 - val_loss: 5.6231e-04\n",
      "Epoch 43/100\n",
      "72000/72000 [==============================] - 222s 3ms/sample - loss: 5.2552e-04 - val_loss: 5.6221e-04\n",
      "Epoch 44/100\n",
      "72000/72000 [==============================] - 230s 3ms/sample - loss: 5.2433e-04 - val_loss: 5.6174e-04\n",
      "Epoch 45/100\n",
      "72000/72000 [==============================] - 243s 3ms/sample - loss: 5.2307e-04 - val_loss: 5.6183e-04\n",
      "Epoch 46/100\n",
      "72000/72000 [==============================] - 246s 3ms/sample - loss: 5.2186e-04 - val_loss: 5.6167e-04\n",
      "Epoch 47/100\n",
      "72000/72000 [==============================] - 244s 3ms/sample - loss: 5.2071e-04 - val_loss: 5.6208e-04\n",
      "Epoch 48/100\n",
      "72000/72000 [==============================] - 215s 3ms/sample - loss: 5.1948e-04 - val_loss: 5.6187e-04\n",
      "Epoch 49/100\n",
      "72000/72000 [==============================] - 224s 3ms/sample - loss: 5.1822e-04 - val_loss: 5.6211e-04\n",
      "Epoch 50/100\n",
      "72000/72000 [==============================] - 248s 3ms/sample - loss: 5.1694e-04 - val_loss: 5.6225e-04\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          #validation_split=0.05,\n",
    "          batch_size=128, epochs=100, callbacks=[callback, modelckpt_callback])\n",
    "\n",
    "model.save('model_attention.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33166036-c4e0-490e-8a79-da85e2755923",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43bc1497-8c23-4995-8215-6f52e5f2e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    clean_input = clean_text(raw_input)\n",
    "    input_tok = [nltk.word_tokenize(clean_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output\n",
    "\n",
    "def decode(decoding, vector):\n",
    "    \"\"\"\n",
    "    :param decoding: decoding dict built by word encoding\n",
    "    :param vector: an encoded vector\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53fc71-cf5a-42cb-9ca8-c16fd124f90a",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136afe24-500d-4269-83a9-3f65efc799d4",
   "metadata": {},
   "source": [
    "## Example 1 with random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e4562a1-27fb-44d1-9a30-bf72c8def46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HBK\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: then it is god, right?\n",
      "A:  i am not sure .\n",
      "Q: rick, i really think i am in love.\n",
      "A:  i am sorry .\n",
      "Q: you sound like the media.\n",
      "A:  i am not sure .\n",
      "Q: do not judge me. you are a joke, coming here from a fuckedup culture, telling us what to do!\n",
      "A:  i am sorry , i am not <UNK> !\n",
      "Q: the new place? new.\n",
      "A:  i do not know .\n",
      "Q: yes. now.\n",
      "A:  i am sorry .\n",
      "Q: i do not see\n",
      "A:  i am not going to be a <UNK> .\n",
      "Q: it is the only name i have got. how about you?\n",
      "A:  i do not know .\n",
      "Q: in a dream.\n",
      "A:  i am sorry .\n",
      "Q: none at this time, sir.\n",
      "A:  i am sorry .\n",
      "Q: then tomorrow's grand jury indictments will just be bullshit.\n",
      "A:  i am sorry , i am not <UNK> .\n",
      "Q: what do you think, honey?\n",
      "A:  i am not sure .\n",
      "Q: it might be paranoia, but i have never lost my keycard before.\n",
      "A:  i am sorry , i am not <UNK> .\n",
      "Q: i read george jean nathan every week.\n",
      "A:  i am sorry .\n",
      "Q: sir, yes, sir!\n",
      "A:  i am sorry , i am not <UNK> !\n",
      "Q: i just cannot leave without first searching those islands, one by one.\n",
      "A:  i am sorry , i am not <UNK> .\n",
      "Q: i live here... guess i do not have to ask what you are doing here.\n",
      "A:  i am not going to <UNK> you , <UNK> .\n",
      "Q: no, i would not even know what he looked like except for the pictures i have been shown.\n",
      "A:  i do not know .\n",
      "Q: sweating like a pig, actually. and yourself?\n",
      "A:  i do not know .\n",
      "Q: how much?\n",
      "A:  i do not know .\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a0af3-3d64-4b32-ba70-845ccc92d1e8",
   "metadata": {},
   "source": [
    "## Example 2 manual input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e25a761-2ad9-4eca-8aec-951a991d02cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am not sure .\n"
     ]
    }
   ],
   "source": [
    "raw_input = input()\n",
    "output = prediction(raw_input)\n",
    "print (decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc3f18-3083-452b-9ba2-096b3ea30792",
   "metadata": {},
   "source": [
    "## Example 3 with random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f9003fa-cee8-4da8-82bb-6a00e041f8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: is he really not here?\n",
      "A:  no rod never me date escort you man . i pretty no stuff is ! kinda .\n",
      "Q: stop it. this is your pain your burning hand. it is right here. look at it.\n",
      "A:  head is push , and i though you .\n",
      "Q: you think that section on the point is ridable, lance?\n",
      "A:  wanted damage find rather exactly know simply .\n",
      "Q: that was fun. i do not think california is bad at all. it is a drag coming home.\n",
      "A:  i listen reputation find cindy .\n",
      "Q: yeah, sure. how much are they?\n",
      "A:  wanted damage find disappeared goes push with following reputation that cuba .\n",
      "Q: that is a great handle.\n",
      "A:  i dry news .\n",
      "Q: just had one.\n",
      "A:  i listen reputation find issue .\n",
      "Q: you want your surprise?\n",
      "A:  and longer ... new you , say for doing listening land .\n",
      "Q: she is quite pretty.\n",
      "A:  i actually .\n",
      "Q: would you? i am not so sure.\n",
      "A:  we dating cry thirsty get results .\n",
      "Q: hell, now's the time to buy it off him cheap.\n",
      "A:  i listen reputation find cindy .\n",
      "Q: you are the one that put us together. we are friends.\n",
      "A:  i listen reputation find religious .\n",
      "Q: sure, evan, why not? you were the first person i really ever cared about.\n",
      "A:  wanted damage be course cover all hey before i go dry hey no head happens . give out goes goes\n",
      "Q: okay, i will not wear your stuff... why cannot i wear your stuff?\n",
      "A:  and you\n",
      "Q: please. you are in danger.\n",
      "A:  i daughters i only always push black .\n",
      "Q: you did not know you had a fairy godmother, did you?\n",
      "A:  and that with you ?\n",
      "Q: how do they know?\n",
      "A:  wanted damage find disappeared goes push with push .\n",
      "Q: no, just a farewell appearance, batting for sweeney. i am going into business for myself.\n",
      "A:  wanted damage find push goes push kinda . that is the gosh the gosh such dog mac take . .\n",
      "Q: and what was this?\n",
      "A:  i listen today , answer , na re .\n",
      "Q: then why go there?\n",
      "A:  you dating push either .\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb37de8-7afc-4f3f-b053-80bbfc4c5542",
   "metadata": {},
   "source": [
    "## Example 4 with random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "637d3e8d-66c6-4748-bea5-b7a83c15d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: okay, so...\n",
      "A:  i am not going to <UNK> you .\n",
      "Q: yeah, but what are you doing out here?\n",
      "A:  i am not sure .\n",
      "Q: in a manner of speaking. i have never encountered the olfactory hallucinations, but i am sure they are related.\n",
      "A:  i am sorry , i am not <UNK> .\n",
      "Q: let me see the sick.\n",
      "A:  i am sorry .\n",
      "Q: give him the works.\n",
      "A:  i do not know .\n",
      "Q: is anybody besides you here now?\n",
      "A:  i am not sure .\n",
      "Q: who is he?\n",
      "A:  he is a <UNK> .\n",
      "Q: wait a minute, sonny. i think you better come with me.\n",
      "A:  i am not going to <UNK> you .\n",
      "Q: do you know what ed gein said about women?\n",
      "A:  i do not know .\n",
      "Q: you sure?\n",
      "A:  i am not sure .\n",
      "Q: not much danger here, ma'am, i would not think right here in the heart of edinburgh.\n",
      "A:  i am sorry , i am not <UNK> .\n",
      "Q: how did you know he was a construction worker?\n",
      "A:  he is a <UNK> .\n",
      "Q: because i did only sleep with three guys! that does not mean i did not just go with people.\n",
      "A:  i am not sure .\n",
      "Q: your daughter's the only leverage they have to keep you quiet.\n",
      "A:  i am not sure .\n",
      "Q: you like mysteries that much?\n",
      "A:  i do not know .\n",
      "Q: yeah. you did good. that is wrong?\n",
      "A:  i am not sure .\n",
      "Q: what are you doing out here alone?\n",
      "A:  i am not sure .\n",
      "Q: when you ran off, i thought you would keep going until you were back home in washington.\n",
      "A:  i am not .\n",
      "Q: there is no news big enough.\n",
      "A:  i am sorry .\n",
      "Q: what is it?\n",
      "A:  i do not know .\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
